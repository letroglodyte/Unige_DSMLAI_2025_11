{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding - Traduction machine naive\n",
    "\n",
    "Dans cet exercice, vous allez mettre en place votre premier syst√®me de traduction automatique!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "from utils import get_dict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "\n",
    "# 1. Embeddings pour mots en Anglais et en Fran√ßais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Le sous-emsemble de donn√©es\n",
    "\n",
    "Pour r√©aliser cet exercice, nous utiliserons une sous-ensemble de word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embeddings_subset = pickle.load(open(\"en_embeddings.p\", \"rb\"))\n",
    "fr_embeddings_subset = pickle.load(open(\"fr_embeddings.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consulter les donn√©es\n",
    "\n",
    "* en_embeddings_subset: la cl√© (key) est un mot en anglais et la valeur est un\n",
    "tableau de 300 dimensions, qui est l'embedding de ce mot.\n",
    "```\n",
    "'the': array([ 0.08007812,  0.10498047,  0.04980469,  0.0534668 , -0.06738281, ....\n",
    "```\n",
    "\n",
    "* fr_embeddings_subset: la cl√© (key) est un mot en fran√ßais et la valeur est un\n",
    "tableau de 300 dimensions, qui est l'embedding de ce mot.\n",
    "```\n",
    "'la': array([-6.18250e-03, -9.43867e-04, -8.82648e-03,  3.24623e-02,...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instruction**: Consultez les premiers √©l√©ments de `en_embeddings_subset` et `fr_embeddings_subset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propri√©t√©s d'Analogie des Embeddings\n",
    "\n",
    "Avant d'attaquer la traduction automatique √† l'aide des embeddings, ouvrons tout d'abord une petite parent√®se afin de d√©montrer quelques propri√©t√©s des embeddings.\n",
    "\n",
    "V√©rifions ensemble que les embeddings de mots peuvent capturer des relations s√©mantiques complexes, permettant de r√©aliser des analogies comme \"paris - france + italie = rome\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(word, embeddings):\n",
    "    \"\"\"\n",
    "    R√©cup√®re l'embedding d'un mot\n",
    "    \n",
    "    Args:\n",
    "        word: Le mot dont on veut l'embedding\n",
    "        embeddings: Dictionnaire des embeddings\n",
    "        \n",
    "    Returns:\n",
    "        Le vecteur d'embedding du mot ou None si le mot n'est pas dans le dictionnaire\n",
    "    \"\"\"\n",
    "    if word in embeddings:\n",
    "        return embeddings[word]\n",
    "    else:\n",
    "        print(f\"Le mot '{word}' n'est pas dans le vocabulaire.\")\n",
    "        return None\n",
    "    \n",
    "def analogy(word1, word2, word3, embeddings, n=5):\n",
    "    \"\"\"\n",
    "    R√©sout une analogie de la forme: word1 est √† word2 ce que ? est word3\n",
    "    Exemple: paris est √† france ce que ? est √† italie -> word1=paris, word2=france, word3=italie\n",
    "    \n",
    "    Args:\n",
    "        word1, word2, word3: Les trois mots de l'analogie\n",
    "        embeddings: Dictionnaire des embeddings\n",
    "        n: Nombre de r√©sultats √† retourner\n",
    "        \n",
    "    Returns:\n",
    "        Liste des n mots les plus proches du r√©sultat de l'analogie\n",
    "    \"\"\"\n",
    "    # R√©cup√©rer les embeddings des mots\n",
    "    emb1 = get_embedding(word1,embeddings)\n",
    "    emb2 = get_embedding(word2,embeddings)\n",
    "    emb3 = get_embedding(word3,embeddings)\n",
    "    \n",
    "    if emb1 is None or emb2 is None or emb3 is None:\n",
    "        return []\n",
    "    \n",
    "    # Calculer le vecteur r√©sultant de l'analogie: word1 - word2 + word3\n",
    "    result_vector = emb1 - emb2 + emb3\n",
    "    \n",
    "    # Trouver les mots les plus proches du vecteur r√©sultant\n",
    "    # Normaliser le vecteur de r√©f√©rence\n",
    "    vector_norm = result_vector / np.linalg.norm(result_vector)\n",
    "    \n",
    "    # Calculer la similarit√© cosinus avec tous les mots\n",
    "    similarities = {}\n",
    "    exclude=[word1, word2, word3]\n",
    "    for word, emb in embeddings.items():\n",
    "        if word in exclude:\n",
    "            continue\n",
    "        emb_norm = emb / np.linalg.norm(emb)\n",
    "        similarity = np.dot(vector_norm, emb_norm)\n",
    "        similarities[word] = similarity\n",
    "    \n",
    "    # Trier les mots par similarit√© d√©croissante\n",
    "    sorted_words = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "     \n",
    "    closest_words = sorted_words[:n]\n",
    "    \n",
    "    return closest_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(analogy('france', 'paris', 'rome', fr_embeddings_subset))\n",
    "\n",
    "# Tester l'analogie avec des mots en fran√ßais\n",
    "None\n",
    "\n",
    "# Tester l'analogie avec des mots en anglais \n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fermons maintenant cette petit parenth√®se et retournons √† notre probl√®me de Traduction Automatique :)\n",
    "\n",
    "#### Charger les deux dictionnaires qui font correspondre les mots anglais aux mots fran√ßais :\n",
    "* Un dictionnaire d'entra√Ænement\n",
    "* Un dictionnaire de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du dictionnaire Anglais -> Fran√ßais\n",
    "en_fr_train = get_dict('en-fr.train.txt')\n",
    "print('The length of the English to French training dictionary is', len(en_fr_train))\n",
    "en_fr_test = get_dict('en-fr.test.txt')\n",
    "print('The length of the English to French test dictionary is', len(en_fr_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consulter le dictionnaire Anglais -> Fran√ßais\n",
    "\n",
    "* `en_fr_train` est un dictionnaire o√π la cl√© est le mot anglais et la valeur\n",
    "est la traduction fran√ßaise de ce mot anglais.\n",
    "```\n",
    "{'the': 'la',\n",
    " 'and': 'et',\n",
    " 'was': '√©tait',\n",
    " 'for': 'pour',\n",
    "```\n",
    "\n",
    "* `en_fr_test` est similaire √† `en_fr_train`, mais pour l'√©tape de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1-1\"></a>\n",
    "\n",
    "## 1.1 Embeddings et matrice de transformation\n",
    "\n",
    "<a name=\"ex-01\"></a>\n",
    "#### Traduction du dictionnaire anglais -> fran√ßais en utilisant des embeddings\n",
    "\n",
    "Vous allez maintenant impl√©menter une fonction `get_matrices`, qui prend les donn√©es charg√©es\n",
    "et renvoie les matrices `X` et `Y`.\n",
    "\n",
    "Entr√©es :\n",
    "- `en_fr` : Dictionnaire anglais -> fran√ßais\n",
    "- `en_embeddings` : Dictionnaire mots anglais vers les embeddings\n",
    "- `fr_embeddings` : Dictionnaire mots fran√ßais vers les embeddings\n",
    "\n",
    "Renvoie :\n",
    "- La matrice `X` et la matrice `Y`, o√π chaque ligne dans X est l'embedding de\n",
    "  d'un mot anglais, et la m√™me ligne dans Y est l'embedding de la version fran√ßaise\n",
    "  de ce mot anglais.\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\">\n",
    "<img src='X_to_Y.jpg' alt=\"texte alternatif\" width=\"largeur\" height=\"hauteur\" style=\"largeur:800px;hauteur:200px;\" /> Figure 2 </div>\n",
    "\n",
    "Utilisez le dictionnaire `en_fr` pour vous assurer que la i-√®me ligne de la matrice `X`\n",
    "correspond √† la i-√®me ligne de la matrice `Y`.\n",
    "\n",
    "<a name=\"1-1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**: Completez la function `get_matrices()`:\n",
    "* R√©cup√©rez les embeddings des mots fran√ßais et anglais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Aide</b></font>\n",
    "</summary>\n",
    "    <p>\n",
    "        <ul>\n",
    "            <li>Utilisez les dictionnaires `french_vecs` et `english_vecs`</li>\n",
    "        </ul>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrices(en_fr, french_vecs, english_vecs):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        en_fr: English to French dictionary\n",
    "        french_vecs: French words to their corresponding word embeddings.\n",
    "        english_vecs: English words to their corresponding word embeddings.\n",
    "    Output: \n",
    "        X: a matrix where the columns are the English embeddings.\n",
    "        Y: a matrix where the columns correspong to the French embeddings.\n",
    "        R: the projection matrix that minimizes the F norm ||X R -Y||^2.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # X_l et Y_l sont les listes d'embedding Anglais et Fran√ßais\n",
    "    X_l = list()\n",
    "    Y_l = list()\n",
    "\n",
    "    # Stocker les mots Anglais du dictionnaire des embeddings Anglais\n",
    "    english_set = set(english_vecs.keys())\n",
    "\n",
    "    # Stocker les mots fran√ßais du dictionnaire des embeddings Fran√ßais\n",
    "    french_set = set(french_vecs.keys())\n",
    "\n",
    "    # Stocker les mots fran√ßais du dictionnaire Anglais-Fran√ßais\n",
    "    french_words = set(en_fr.values())\n",
    "\n",
    "    # Boucle sur toutes les paires de mots anglais et fran√ßais dans le dictionnaire anglais-fran√ßais.\n",
    "\n",
    "    for en_word, fr_word in en_fr.items():\n",
    "\n",
    "        # V√©rifie que le mot fran√ßais a un embedding et que le mot anglais a un embedding.\n",
    "        if fr_word in french_set and en_word in english_set:\n",
    "\n",
    "            ### Code ici - D√©but\n",
    "            # Recuperez l'embedding du mot anglais\n",
    "            en_vec = None\n",
    "\n",
    "            # Recuperez l'embedding du mot fran√ßais\n",
    "            fr_vec = None\n",
    "            ### Code ici - Fin\n",
    "\n",
    "            # Ajoute l'embedding anglais √† la liste\n",
    "            X_l.append(en_vec)\n",
    "\n",
    "            # Ajoute l'embedding fran√ßais √† la liste\n",
    "            Y_l.append(fr_vec)\n",
    "\n",
    "    # Empile les vecteurs X_l dans une matrice X\n",
    "    X = np.vstack(X_l)\n",
    "\n",
    "    # Empile les vecteurs Y_l dans une matrice Y\n",
    "    Y = np.vstack(Y_l)\n",
    "\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©cup√©re l'ensemble d'entrainement\n",
    "X_train, Y_train = get_matrices(\n",
    "    en_fr_train, fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "\n",
    "# 2. Traduction\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='e_to_f.jpg' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:700px;height:200px;\" /> Figure 1 </div>\n",
    "\n",
    "<a name=\"2-1\"></a>\n",
    "## 2.1 Traduction comme transformation lin√©aire des embeddings\n",
    "\n",
    "√âtant donn√© des dictionnaires d'embedding de mots anglais et fran√ßais, vous cr√©erez une matrice de transformation `R`.\n",
    "* √âtant donn√© un embedding de mot anglais, $\\mathbf{e}$, vous pouvez le multiplier par $\\mathbf{eR}$ pour obtenir un nouvel embedding de mot $\\mathbf{f}$.\n",
    "    * √Ä la fois $\\mathbf{e}$ et $\\mathbf{f}$ sont des [vecteurs ligne](https://en.wikipedia.org/wiki/Row_and_column_vectors).\n",
    "* Vous pouvez ensuite calculer les voisins les plus proches de `f` dans les embeddings fran√ßais et recommander le mot le plus similaire √† l'embedding de mot transform√©."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trouver une matrice `R` qui minimise l'√©quation suivante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction de co√ªt \n",
    "\n",
    "$$ \\frac{1}{m} \\|  \\mathbf{X R} - \\mathbf{Y} \\|_{F}^{2}$$\n",
    "\n",
    "o√π $m$ est le nombre d'exemples (nombre de lignes dans $\\mathbf{X}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex-02\"></a>\n",
    "\n",
    "#### Impl√©mentation du m√©canisme de traduction.\n",
    "\n",
    "#### √âtape 1 : Calcul du co√ªt\n",
    "* La fonction de co√ªt :\n",
    "$$ L(X, Y, R)=\\frac{1}{m}\\sum_{i=1}^{m} \\sum_{j=1}^{n}\\left( a_{i j} \\right)^{2}$$\n",
    "\n",
    "o√π $a_{i j}$ est la valeur dans la $i$-√®me ligne et la $j$-√®me colonne de la matrice $\\mathbf{XR}-\\mathbf{Y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X, Y, R):\n",
    "    '''\n",
    "    Inputs: \n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
    "    Outputs:\n",
    "        L: a matrix of dimension (m,n) - the value of the loss function for given X, Y and R.\n",
    "    '''\n",
    "    # m is the number of rows in X\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # diff is XR - Y\n",
    "    diff = np.dot(X,R)-Y\n",
    "\n",
    "    # diff_squared is the element-wise square of the difference\n",
    "    diff_squared = np.square(diff)\n",
    "\n",
    "    # sum_diff_squared is the sum of the squared elements\n",
    "    sum_diff_squared = np.sum(diff_squared)\n",
    "\n",
    "    # loss i the sum_diff_squard divided by the number of examples (m)\n",
    "    loss = sum_diff_squared/m\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √âtape 2 : Calcul du gradient du co√ªt par rapport √† la matrice de transformation R\n",
    "\n",
    "* Calculer le gradient de la perte par rapport √† la matrice de transformation `R`.\n",
    "* Le gradient est une matrice qui indique dans quelle mesure un petit changement dans `R` affecte le changement de la fonction de perte.\n",
    "* Le gradient nous donne la direction dans laquelle nous devons diminuer `R` pour minimiser la fonction de co√ªt.\n",
    "* $m$ est le nombre d'exemples d'entra√Ænement (nombre de lignes dans $X$).\n",
    "* La formule du gradient de la fonction de perte $ùêø(ùëã,ùëå,ùëÖ)$ est la suivante :\n",
    "\n",
    "$$\\frac{d}{dR}ùêø(ùëã,ùëå,ùëÖ)=\\frac{d}{dR}\\Big(\\frac{1}{m}\\| X R -Y\\|_{F}^{2}\\Big) = \\frac{2}{m}X^{T} (X R - Y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, Y, R):\n",
    "    '''\n",
    "    Inputs: \n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
    "    Outputs:\n",
    "        g: a matrix of dimension (n,n) - gradient of the loss function L for given X, Y and R.\n",
    "    '''\n",
    "    # m is the number of rows in X\n",
    "    m = X.shape[0]\n",
    "\n",
    "    # gradient is X^T(XR - Y) * 2/m\n",
    "    gradient = np.dot(X.T,np.dot(X,R)-Y)*2/m\n",
    "    return gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √âtape 3 : Recherche de la meilleure matrice R avec l'algorithme de descente de gradient\n",
    "\n",
    "#### Descente de gradient\n",
    "\n",
    "La [descente de gradient](https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html) est un algorithme it√©ratif utilis√© pour rechercher l'optimum d'une fonction. \n",
    "* Comme mentionn√© pr√©c√©demment, le gradient de la perte par rapport √† la matrice indique dans quelle mesure un petit changement dans une coordonn√©e de cette matrice affecte le changement de la fonction de perte.\n",
    "* La descente de gradient utilise cette information pour changer de mani√®re it√©rative la matrice `R` jusqu'√† ce que nous atteignions un point o√π la perte est minimis√©e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entra√Ænement avec un nombre fixe d'it√©rations\n",
    "\n",
    "La plupart du temps, nous it√©rons pour un nombre fixe d'√©tapes d'entra√Ænement plut√¥t que d'it√©rer jusqu'√† ce que la perte descende en dessous d'un seuil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo-code :\n",
    "1. Calculer le gradient $g$ de la perte par rapport √† la matrice $R$.\n",
    "2. Mettre √† jour $R$ avec la formule :\n",
    "$$R_{\\text{new}}= R_{\\text{old}}-\\alpha g$$\n",
    "\n",
    "O√π $\\alpha$ est le taux d'apprentissage, qui est un scalaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Le taux d'apprentissage ou \"pas\" $\\alpha$ est un coefficient qui d√©cide combien nous voulons changer $R$ √† chaque √©tape.\n",
    "* Si nous changeons $R$ trop rapidement, nous pourrions passer √† c√¥t√© de l'optimum en prenant un pas trop grand.\n",
    "* Si nous apportons seulement de petits changements √† $R$, nous aurons besoin de nombreuses √©tapes pour atteindre l'optimum.\n",
    "* Le taux d'apprentissage $\\alpha$ est utilis√© pour contr√¥ler ces changements.\n",
    "* Les valeurs de $\\alpha$ sont choisies en fonction du probl√®me, et nous utiliserons `learning_rate`$=0.0003$ comme valeur par d√©faut pour notre algorithme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions: Implementez `align_embeddings()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Aide</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Utilisez la fonction 'compute_gradient()' pour obtenir le gradient √† chaque it√©ration</li>\n",
    "\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_embeddings(X, Y, train_steps=100, learning_rate=0.0003):\n",
    "    '''\n",
    "    Inputs:\n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        train_steps: positive int - describes how many steps will gradient descent algorithm do.\n",
    "        learning_rate: positive float - describes how big steps will  gradient descent algorithm do.\n",
    "    Outputs:\n",
    "        R: a matrix of dimension (n,n) - the projection matrix that minimizes the F norm ||X R -Y||^2\n",
    "    '''\n",
    "    np.random.seed(129)\n",
    "\n",
    "    # Nombre de lignes et de colonnes de R est √©gal au nombre de dimensions de l'embedding (e.g. 300)\n",
    "    R = np.random.rand(X.shape[1], X.shape[1])\n",
    "\n",
    "    for i in range(train_steps):\n",
    "        if i % 25 == 0:\n",
    "            print(f\"loss at iteration {i} is: {compute_loss(X, Y, R):.4f}\")\n",
    "        ### Code ici - D√©but\n",
    "        # Utilisez la fonction impl√©ment√©e plus haut et qui permet de calculer le gradient\n",
    "        gradient = None\n",
    "\n",
    "        # Mettre √† jour R en soustrayant le taux d'apprentissage fois le gradient\n",
    "        R = None\n",
    "        ### Code ici - Fin\n",
    "    return R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester l'impl√©mentation\n",
    "np.random.seed(129)\n",
    "m = 10\n",
    "n = 5\n",
    "X = np.random.rand(m, n)\n",
    "Y = np.random.rand(m, n) * .1\n",
    "R = align_embeddings(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "loss at iteration 0 is: 3.7242\n",
    "loss at iteration 25 is: 3.6283\n",
    "loss at iteration 50 is: 3.5350\n",
    "loss at iteration 75 is: 3.4442\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculer la matrice de transformation R\n",
    "\n",
    "Calculer la matrice de transformation R, en utilisant l'ensemble d'entra√Ænement et en appelant la fonction align_embeddings().\n",
    "\n",
    "REMARQUE : La cellule de code ci-dessous prendra quelques minutes pour s'ex√©cuter compl√®tement (~3 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_train = align_embeddings(X_train, Y_train, train_steps=400, learning_rate=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Expected Output\n",
    "\n",
    "```\n",
    "loss at iteration 0 is: 963.0146\n",
    "loss at iteration 25 is: 46.9876\n",
    "loss at iteration 50 is: 7.1546\n",
    "loss at iteration 75 is: 1.8814\n",
    "loss at iteration 100 is: 0.9121\n",
    "loss at iteration 125 is: 0.6777\n",
    "loss at iteration 150 is: 0.6070\n",
    "loss at iteration 175 is: 0.5820\n",
    "loss at iteration 200 is: 0.5723\n",
    "loss at iteration 225 is: 0.5682\n",
    "loss at iteration 250 is: 0.5663\n",
    "loss at iteration 275 is: 0.5655\n",
    "loss at iteration 300 is: 0.5651\n",
    "loss at iteration 325 is: 0.5649\n",
    "loss at iteration 350 is: 0.5648\n",
    "loss at iteration 375 is: 0.5647\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Tester la traduction\n",
    "\n",
    "### Algorithme des k-plus proches voisins (k-NN)\n",
    "\n",
    "[L'algorithme des k-plus proches voisins](https://fr.wikipedia.org/wiki/K-plus_proches_voisins)\n",
    "* L'algorithme k-NN est une m√©thode qui prend un vecteur en entr√©e et trouve les autres vecteurs du jeu de donn√©es qui lui sont les plus proches.\n",
    "* Le \"k\" repr√©sente le nombre de \"plus proches voisins\" √† trouver (par exemple, k=2 trouve les deux voisins les plus proches).\n",
    "\n",
    "### Recherche de l'embedding de la traduction\n",
    "Puisque nous approximons la fonction de traduction des embeddings anglais vers les embeddings fran√ßais par une matrice de transformation lin√©aire $\\mathbf{R}$, la plupart du temps, nous n'obtiendrons pas l'embedding exacte d'un mot fran√ßais lorsque nous transformons l'embedding $\\mathbf{e}$ d'un mot anglais dans l'espace d'embedding fran√ßais.\n",
    "* C'est l√† que k-NN devient utile ! En utilisant le 1-NN avec $\\mathbf{eR}$ comme entr√©e, nous pouvons rechercher un embedding $\\mathbf{f}$ (sous forme de ligne) dans la matrice $\\mathbf{Y}$ qui est la plus proche du vecteur transform√© $\\mathbf{eR}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarit√© cosinus\n",
    "La similarit√© cosinus entre les vecteurs $u$ et $v$ est calcul√©e comme le cosinus de l'angle entre eux.\n",
    "La formule est la suivante :\n",
    "\n",
    "$$\\cos(u,v) = \\frac{u \\cdot v}{\\left\\|u\\right\\| \\left\\|v\\right\\|}$$\n",
    "* $\\cos(u,v)$ = $1$ lorsque $u$ et $v$ se trouvent sur la m√™me ligne et ont la m√™me direction.\n",
    "* $\\cos(u,v)$ est $-1$ lorsqu'ils ont des directions exactement oppos√©es.\n",
    "* $\\cos(u,v)$ est $0$ lorsque les vecteurs sont orthogonaux (perpendiculaires) les uns aux autres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor(v, candidates, k=1):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      - v, the vector you are going find the nearest neighbor for\n",
    "      - candidates: a set of vectors where we will find the neighbors\n",
    "      - k: top k nearest neighbors to find\n",
    "    Output:\n",
    "      - k_idx: the indices of the top k closest vectors in sorted form\n",
    "    \"\"\"\n",
    "    similarity_l = []\n",
    "\n",
    "    # for each candidate vector...\n",
    "    for row in candidates:\n",
    "        # get the cosine similarity\n",
    "        cos_similarity = np.dot(v,row)/(np.linalg.norm(v)*np.linalg.norm(row))\n",
    "\n",
    "        # append the similarity to the list\n",
    "        similarity_l.append(cos_similarity)\n",
    "     \n",
    "    # sort the similarity list and get the indices of the sorted list\n",
    "    sorted_ids = np.argsort(similarity_l)\n",
    "    \n",
    "    # get the indices of the k most similar candidate vectors\n",
    "    k_idx = sorted_ids[-k:]\n",
    "    \n",
    "    return k_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testez l'implementation du k-nn\n",
    "v = np.array([1, 0, 1])\n",
    "candidates = np.array([[1, 0, 5], [-2, 5, 3], [2, 0, 1], [6, -9, 5], [9, 9, 9]])\n",
    "print(candidates[nearest_neighbor(v, candidates, 3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "`[[9 9 9]\n",
    " [1 0 5]\n",
    " [2 0 1]]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testez votre traduction et calculez sa pr√©cision\n",
    "\n",
    "* Parcourez les embeddings de mots en anglais transform√©s et v√©rifiez si le vecteur de mot fran√ßais le plus proche appartient au mot fran√ßais qui est la traduction r√©elle.\n",
    "* Obtenez un indice de l'embedding fran√ßais le plus proche en utilisant `nearest_neighbor` (avec l'argument `k=1`), et comparez-le √† l'indice de l'embedding en anglais que vous venez de transformer.\n",
    "* Gardez une trace du nombre de fois o√π vous obtenez la traduction correcte.\n",
    "* Calculez la pr√©cision comme suit : $$\\text{pr√©cision} = \\frac{\\#(\\text{pr√©dictions correctes})}{\\#(\\text{pr√©dictions totales})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vocabulary(X, Y, R):\n",
    "    '''\n",
    "    Input:\n",
    "        X: a matrix where the columns are the English embeddings.\n",
    "        Y: a matrix where the columns correspong to the French embeddings.\n",
    "        R: the transform matrix which translates word embeddings from\n",
    "        English to French word vector space.\n",
    "    Output:\n",
    "        accuracy: for the English to French capitals\n",
    "    '''\n",
    "\n",
    "    # The prediction is X times R\n",
    "    pred = np.dot(X,R)\n",
    "\n",
    "    # initialize the number correct to zero\n",
    "    num_correct = 0\n",
    "\n",
    "    # loop through each row in pred (each transformed embedding)\n",
    "    for i in range(len(pred)):\n",
    "        # get the index of the nearest neighbor of pred at row 'i'; also pass in the candidates in Y\n",
    "        pred_idx = nearest_neighbor(pred[i], Y, k=1)\n",
    "\n",
    "        # if the index of the nearest neighbor equals the row of i... \\\n",
    "        if pred_idx == i:\n",
    "            # increment the number correct by 1.\n",
    "            num_correct += 1\n",
    "\n",
    "    # accuracy is the number correct divided by the number of rows in 'pred' (also number of rows in X)\n",
    "    accuracy = num_correct/pred.shape[0]\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelle performance sur les donn√©es de test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = test_vocabulary(X_val, Y_val, R_train)  # this might take a minute or two\n",
    "print(f\"accuracy on test set is {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "```\n",
    "0.561\n",
    "```\n",
    "### R√©sum√©\n",
    "Vous avez r√©ussi √† traduire des mots d'une langue √† une autre sans les avoir jamais vus avec une pr√©cision de presque 56% en utilisant de l'alg√®bre lin√©aire de base et en apprenant une correspondance entre les mots d'une langue √† une autre gr√¢ce aux embeddings!"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "NLPC1-4"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
